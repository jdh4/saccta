{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0529c814-19e2-40f3-8ce0-80b9a4abb282",
   "metadata": {},
   "source": [
    "# Should we convert 80GB A100 GPUs into two 40 GB MIG instances?\n",
    "### April 21, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee223f15-d63f-4c99-86e0-0697ba7c264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "# wget https://raw.githubusercontent.com/PrincetonUniversity/job_defense_shield/refs/heads/main/efficiency.py\n",
    "from efficiency import get_stats_dict\n",
    "from efficiency import cpu_memory_usage\n",
    "from efficiency import gpu_memory_usage_eff_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2e0d8c-6190-48f9-b2f9-e56fe81069e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert slurm timestamps to seconds\n",
    "os.environ[\"SLURM_TIME_FORMAT\"] = \"%s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eccad6a-aa69-4236-becd-d8ecf6a37a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_sacct(clusters: str,\n",
    "                        start_date: str,\n",
    "                        end_date: str,\n",
    "                        partitions: str,\n",
    "                        fields: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a dataframe of the sacct output.\"\"\"\n",
    "    cmd = f\"sacct -M {clusters} -a -X -P -n -S {start_date} -E {end_date} {partitions} -o {fields}\"\n",
    "    output = subprocess.run(cmd,\n",
    "                            stdout=subprocess.PIPE,\n",
    "                            shell=True,\n",
    "                            timeout=100,\n",
    "                            text=True,\n",
    "                            check=True)\n",
    "    rows = [row.split(\"|\") for row in output.stdout.split()]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.columns = fields.split(\",\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26567c62-bf16-45e8-ada9-85aaa2d111cf",
   "metadata": {},
   "source": [
    "### Ignore \"gputest\" partition since those jobs also run on cryoem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5bf39ec-4275-4dbb-9b3c-81cf644bf575",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = \"della\"\n",
    "partitions = f\"-r gpu,gpu-shared\"\n",
    "fields = \"alloctres,elapsedraw,start\"\n",
    "start_date = \"2025-02-01\"\n",
    "end_date = \"now\"\n",
    "fields = \"jobid,cluster,user,alloctres,elapsedraw,admincomment,ncpus\"\n",
    "df = get_data_from_sacct(clusters, start_date, end_date, partitions, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dff88c1-300d-480c-acb8-f4f0ed402817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notna(df.elapsedraw)]\n",
    "df = df[df.elapsedraw.str.isnumeric()]\n",
    "df.elapsedraw = df.elapsedraw.astype(\"int64\")\n",
    "df = df[df.elapsedraw > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc57fce7-7637-4600-bd97-4c7d63decb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notna(df.ncpus)]\n",
    "df = df[df.ncpus.str.isnumeric()]\n",
    "df.ncpus = df.ncpus.astype(\"int64\")\n",
    "df = df[df.ncpus > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a21b67-2412-47f9-9448-f112b09a5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpus_per_job(tres: str) -> int:\n",
    "    \"\"\"Return the number of allocated GPUs.\"\"\"\n",
    "    gpus = re.findall(r\"gres/gpu=\\d+\", tres)\n",
    "    return int(gpus[0].replace(\"gres/gpu=\", \"\")) if gpus else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a215b9-8fc8-44ab-b461-0505b5de2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gpus\"] = df.alloctres.apply(gpus_per_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b20dc63-67f3-4456-a10c-ed4091645936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gpu-seconds\"] = df.apply(lambda row: row[\"elapsedraw\"] * row[\"gpus\"], axis='columns')\n",
    "df[\"gpu-hours\"] = df[\"gpu-seconds\"] / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aee126-8f71-469d-aa06-b5bc2bb6ecff",
   "metadata": {},
   "source": [
    "### Percent Usage of Public GPUs on Della"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6d82a64-838f-4f09-89ea-f0dbc1805527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Usage = 93%\n"
     ]
    }
   ],
   "source": [
    "days = 28 + 31 + 20  # feb, march, part of april\n",
    "hours_per_day = 24\n",
    "num_gpus = 20 * 2 + 69 * 4\n",
    "pct_usage = df[\"gpu-hours\"].sum() / (days * hours_per_day * num_gpus)\n",
    "print(f\"Percent Usage = {round(100 * pct_usage)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066494f-d2da-48c8-a934-f8d7fcc90b1d",
   "metadata": {},
   "source": [
    "We see that 93% of the available GPU-hours were consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e11d55-1c3e-495e-a5d8-ffa54a3053ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"admincomment\"] = df[\"admincomment\"].apply(get_stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "941ec361-f68e-41aa-9a06-fc559e34c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gpu-tuple\"] = df.apply(lambda row: gpu_memory_usage_eff_tuples(row[\"admincomment\"],\n",
    "                                                                   row[\"jobid\"],\n",
    "                                                                   row[\"cluster\"],\n",
    "                                                                   verbose=False),\n",
    "                                                                   axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2af7256f-086c-408f-bf3e-402c0e692e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"error_code\"] = df[\"gpu-tuple\"].apply(lambda x: x[1])\n",
    "df = df[df[\"error_code\"] == 0]\n",
    "df[\"GPU-Mem-Used\"] = df[\"gpu-tuple\"].apply(lambda tpl: tpl[0][0][0])\n",
    "df[\"GPU-Util\"]     = df[\"gpu-tuple\"].apply(lambda tpl: tpl[0][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceafc2fa-865e-413d-adc8-e4add13272f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"error_code\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b072aff-d4d0-49d7-a9b2-68cdeec6dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"memory-tuple\"] = df.apply(lambda row: cpu_memory_usage(row[\"admincomment\"],\n",
    "                                                           row[\"jobid\"],\n",
    "                                                           row[\"cluster\"],\n",
    "                                                           verbose=False),\n",
    "                                                           axis=\"columns\")\n",
    "cols = [\"CPU-Mem-Used\", \"mem-alloc\", \"error_code\"]\n",
    "df[cols] = pd.DataFrame(df[\"memory-tuple\"].tolist(), index=df.index)\n",
    "df = df[df[\"error_code\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a5b978d-e2f8-4824-8f74-2b2912d32afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cores_per_gpu\"] = df.ncpus / df.gpus\n",
    "df[\"CPU-Mem-Used-per-GPU\"] = df[\"CPU-Mem-Used\"] / df.gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1095b45-b26b-47ed-8e72-03637dfce829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_gpu_mem(tpl):\n",
    "    items, error_code = tpl\n",
    "    return max([item[0] for item in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dfdfb2b-111d-45a4-ada4-a53d2ed934be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_gpu_util(tpl):\n",
    "    items, error_code = tpl\n",
    "    return max([item[2] for item in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b555a70f-20e8-4eed-853d-0faf425bcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"max_gpu_mem\"]  = df[\"gpu-tuple\"].apply(max_gpu_mem)\n",
    "df[\"max_gpu_util\"] = df[\"gpu-tuple\"].apply(max_gpu_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a72e57b7-4734-48bf-aab8-5bcfc5af6416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jobid</th>\n",
       "      <td>61797715_0</td>\n",
       "      <td>61797715_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <td>della</td>\n",
       "      <td>della</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>hyen</td>\n",
       "      <td>hyen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alloctres</th>\n",
       "      <td>billing=8192,cpu=8,gres/gpu=2,mem=100G,node=1</td>\n",
       "      <td>billing=8192,cpu=8,gres/gpu=2,mem=100G,node=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elapsedraw</th>\n",
       "      <td>149</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admincomment</th>\n",
       "      <td>{'nodes': {'della-l03g1': {'total_memory': 107...</td>\n",
       "      <td>{'nodes': {'della-l03g1': {'total_memory': 107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncpus</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpus</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu-seconds</th>\n",
       "      <td>298</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu-hours</th>\n",
       "      <td>0.082778</td>\n",
       "      <td>0.337222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu-tuple</th>\n",
       "      <td>([(1.2, 80.0, 0.0), (5.4, 80.0, 3.4)], 0)</td>\n",
       "      <td>([(7.4, 80.0, 0.2), (7.4, 80.0, 0.9)], 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU-Mem-Used</th>\n",
       "      <td>1.2</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU-Util</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory-tuple</th>\n",
       "      <td>(0.0, 100.0, 0)</td>\n",
       "      <td>(1.0, 100.0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPU-Mem-Used</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mem-alloc</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>error_code</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cores_per_gpu</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPU-Mem-Used-per-GPU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_gpu_mem</th>\n",
       "      <td>5.4</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_gpu_util</th>\n",
       "      <td>3.4</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    282  \\\n",
       "jobid                                                        61797715_0   \n",
       "cluster                                                           della   \n",
       "user                                                               hyen   \n",
       "alloctres                 billing=8192,cpu=8,gres/gpu=2,mem=100G,node=1   \n",
       "elapsedraw                                                          149   \n",
       "admincomment          {'nodes': {'della-l03g1': {'total_memory': 107...   \n",
       "ncpus                                                                 8   \n",
       "gpus                                                                  2   \n",
       "gpu-seconds                                                         298   \n",
       "gpu-hours                                                      0.082778   \n",
       "gpu-tuple                     ([(1.2, 80.0, 0.0), (5.4, 80.0, 3.4)], 0)   \n",
       "GPU-Mem-Used                                                        1.2   \n",
       "GPU-Util                                                            0.0   \n",
       "memory-tuple                                            (0.0, 100.0, 0)   \n",
       "CPU-Mem-Used                                                        0.0   \n",
       "mem-alloc                                                         100.0   \n",
       "error_code                                                            0   \n",
       "cores_per_gpu                                                       4.0   \n",
       "CPU-Mem-Used-per-GPU                                                0.0   \n",
       "max_gpu_mem                                                         5.4   \n",
       "max_gpu_util                                                        3.4   \n",
       "\n",
       "                                                                    283  \n",
       "jobid                                                        61797715_1  \n",
       "cluster                                                           della  \n",
       "user                                                               hyen  \n",
       "alloctres                 billing=8192,cpu=8,gres/gpu=2,mem=100G,node=1  \n",
       "elapsedraw                                                          607  \n",
       "admincomment          {'nodes': {'della-l03g1': {'total_memory': 107...  \n",
       "ncpus                                                                 8  \n",
       "gpus                                                                  2  \n",
       "gpu-seconds                                                        1214  \n",
       "gpu-hours                                                      0.337222  \n",
       "gpu-tuple                     ([(7.4, 80.0, 0.2), (7.4, 80.0, 0.9)], 0)  \n",
       "GPU-Mem-Used                                                        7.4  \n",
       "GPU-Util                                                            0.2  \n",
       "memory-tuple                                            (1.0, 100.0, 0)  \n",
       "CPU-Mem-Used                                                        1.0  \n",
       "mem-alloc                                                         100.0  \n",
       "error_code                                                            0  \n",
       "cores_per_gpu                                                       4.0  \n",
       "CPU-Mem-Used-per-GPU                                                0.5  \n",
       "max_gpu_mem                                                         7.4  \n",
       "max_gpu_util                                                        0.9  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.gpus > 1].head(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1f743-b7e7-4995-a04c-833c67dc9661",
   "metadata": {},
   "source": [
    "The nodes with 80 GB A100's have 1000 GB of CPU memory, 48 cores and 4 GPUs. If split in half then would have 6 cores per GPU, 125 GB of CPU memory and 40 GB of GPU memory. The percentage of the GPU-hours that could run on these instances is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69919ccd-9c60-4f0b-b1b5-baf116efe5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of GPU-hours that could run on 40 GB MIG instance = 21%\n"
     ]
    }
   ],
   "source": [
    "df = df[(df[\"cores_per_gpu\"] <= 6) & (df[\"CPU-Mem-Used-per-GPU\"] < 125) & (df[\"max_gpu_util\"] < 50) & (df[\"max_gpu_mem\"] < 40)]\n",
    "pct = f\"Percentage of GPU-hours that could run on 40 GB MIG instance = {round(100 * df[\"gpu-hours\"].sum() / (days * hours_per_day * num_gpus))}%\"\n",
    "print(pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce216a-342f-4315-aa6d-a25a8613b926",
   "metadata": {},
   "source": [
    "Note that the 21% is a lower bound because some jobs use less than 40 GB of GPU memory and up to 64 CPU-cores and 384 of CPU memory. If one uses thresholds of 64 cores, 384GB CPU, 40GB GPU, <50% GPU util, then the percentage is 26%. Also, for multinode jobs, we use the max memory usage per GPU and the max utilization per GPU.\n",
    "\n",
    "Another interesting set of thresholds is to ignore GPU utilization and just look at <6 core, < 125 GB CPU, and < 40GB GPU. In this case the number is 63%.\n",
    "\n",
    "Unfortunately, we do not have SM% or occupancy which are much better indicators of how much of the GPU is being used. A code with high GPU utilization and low SM% will run just as fast on a MIG instance.\n",
    "\n",
    "Let's take the value as 24%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eaef8b-ba76-42ce-90d0-47b8966c9bb1",
   "metadata": {},
   "source": [
    "There are 316 A100 GPUs in the public pool on Della. There are already 40 GPUs with only 40GB which can account for about half of the needed 24%. An additional 36 40GB GPUs are needed to cover the entire 24%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d113a4b-0937-4f20-90af-a77de335f542",
   "metadata": {},
   "source": [
    "# Final Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc03d5-b238-4c95-9542-b903ed8bc8a7",
   "metadata": {},
   "source": [
    "Convert between 16 to 20 of the 80GB A100s (4 or 5 nodes) into between 32 to 40 40GB MIG instances. If all goes well then we could convert more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
